---
title: "Practical Machine Learning Course Project"
author: "Jacques Belissent"
date: "September 21, 2014"
output: html_document
---


Data exploration
---------------------------------------------------------------------------------

(make sure you change the directory to that where the data has been extracted, using setwd())
```{r}
setwd("/Users/jbelis/Documents/pml")
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
library(caret)
```

A number of variables are never available in the test data set.  We therefore generate a new training data set excluding these variables.

```{r}
useful_cols = sapply(testing, function(x) !all(is.na(x)))
```

In addition to these columns, we exclude the following columns that have no prediction value:
- X : test index, assumed to be artifically correlated with the class variable
- user_name : uncorrelated, assuming any correlation would be artificial
- raw_timestamp_part_1 : discrete timestamp
- raw_timestamp_part_2 : integer with stable distribution
- cvtd_timestamp : discrete date/time
- new_window : constant in test set
- num_window : integer assumed to be artifically correlated with the class variable

So the data sets are hereby reduced:
```{r}
useful_cols[1:7] = FALSE
training1 = training[,useful_cols]
testing1 = testing[,useful_cols]
```


Dataset Slicing
---------------------------------------------------------------------------------

The test data does not include the class variable.  We therefore have to split the training set into training and model validation data sets, by setting out 30% of records for validation

```{r}
set.seed(123)
inTrain = createDataPartition(y=training1$classe, p=.7, list=FALSE)
cv1 = training1[-inTrain,]
train1 = training1[inTrain,]
```

Preprocessing
---------------------------------------------------------------------------------

One side benefit of removing columns during data exploration is that we end up with a data set without missing data (which was not the case to begin with), and consisting only of numerical data.  Normalization is handled as part or training.  The data set does not include near-zero-variables, and while there are highly correlated variables, there is no a uge amount of them.

I spent some time experimenting with PCA but the experiment did not yield very high quality predictors, as illustrated in the plot below.  The 2 most critical PCA components plotted against each other do not succeed in separating the classes. 

```{r}
preProc = preProcess(train1[,-53], method="pca", pcaComp=2)
pca1 = predict(preProc, train1[,-53])
featurePlot(x=pca1, y=train1$classe, plot="pairs")
```



Model selection
---------------------------------------------------------------------------------

I am not going to bore you too much with how I arrived at the conclusion that using boosting provided a vastly better accuracy.  In the process, I tried what feels like a large number of classifiers to the chagrin of my overheating laptop.  All this despite the fact that the first line of the lecture notes on boosting is

> 1. Take lots of (possibly) weak predictors

And what we have is precisely a set of 52 week predictors, as shown by the results of an initial decision tree prediction attempt: a 55% accuracy rate.

```{r}
treefit = train(classe ~ ., method="rpart", preProcess=c("center","scale"), data=train1)
confusionMatrix(predict(treefit, newdata=cv1), cv1$classe)
treefit$finalModel
```
A look at the tree model above shows that several leaf nodes, especially for classes B, C, D do not lead to high accuracy.

Fortunately, much better results are obtained using boosting with trees (gbm).

Warning: This steps takes a really long time.  It took 20 minutes on my fairly recent laptop.  It may be sufficient to use a smaller subset but I'd need more time to determine at what point it would start impacting prediction quality.

```{r}
date()
fit = train(classe ~ ., method="gbm", preProcess=c("center","scale"), data=train1, verbose=FALSE)
date()
confusionMatrix(predict(fit, newdata=cv1), cv1$classe)
```

The 96% accuracy is achieved for a tree depth of 3 and 150 iterations.
```{r}
plot(fit)
```



Prediction
---------------------------------------------------------------------------------

The following is the prediction vector for the test set, using the model created above.
```{r}
submission = predict(fit, newdata=testing1)
submission
```

The validation set model accuracy is in the order of 95% (19 out of 20 correct).  The test data set may have a bias so the stated accuracy is really an upper bound for the test set accuracy.  More importantly, It should noted that the test set is very small, and therefore subject to high variance.  This means a sizeable margin of error should be applied.  I'd question whether a test set of 20 suffices in assessing the quality of the model.

